{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1de40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from dataset import load_dataset\n",
    "\n",
    "from hypernetwork import TaskWeaver\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3210ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9f12e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model: AutoModelForCausalLM, tokenizer, prompt, **gen_kwargs) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "    outputs = model.generate(**inputs, **gen_kwargs).squeeze(0).cpu()\n",
    "    response = tokenizer.decode(outputs)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2501e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base language model: EleutherAI/pythia-70M-deduped\n",
      "TaskWeaver hypernetwork loaded from ./taskweaver_output\n"
     ]
    }
   ],
   "source": [
    "hypernet = TaskWeaver.from_pretrained('./taskweaver_output').to(device)\n",
    "model = AutoModelForCausalLM.from_pretrained(hypernet.model_name, device_map=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hypernet.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69ea1b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('openai/gsm8k', 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6df55be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? And they just had a chance to see how many clips, or what do they have in common? And when did they come from those clips? For the first time in the past two years, Natalia has been doing the same thing in the past two years.\n",
      "\n",
      "But what about the other clips? Natalia had her own video clips, but he had her own videos, and he had her own clips on the last year. She gave the clips her own clips as a gift, but then she was having them on her videos.\n",
      "\n",
      "To her, Natalia's best friend, Natalia is one of the most important\n",
      "HN Response: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?After four of them sold, Natalia sold half-as many clips in May to 52 + 48 + 48 = <<52+48+48=12>>12 clips of her friends in April.\n",
      "After two clips, Natalia sold half-as many clips in May to 12 + 12 = <<12+12+12=12>>12 clips of her friends in April.\n",
      "#### 12+12+12+24 = <<12+12+24=72>>72 clips of her friends in April.\n",
      "#### 72+72+72+72+72+72+72+72+72+72+72+\n"
     ]
    }
   ],
   "source": [
    "prompt = dataset['train'][0]['question']\n",
    "print(f\"Prompt: {prompt}\")\n",
    "\n",
    "\n",
    "model_response = generate(model, tokenizer, prompt, max_new_tokens=128, do_sample=True, temperature=0.7)\n",
    "print(f\"Model Response: {model_response}\")\n",
    "hn_response = generate(hypernet, tokenizer, prompt, max_new_tokens=128, do_sample=True, temperature=0.7)\n",
    "print(f\"HN Response: {hn_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "534d67ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The capital of Paris is \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Response: The capital of Paris is \n",
      "\n",
      "L'Unionle du GÃ©minaire,\n",
      "\n",
      "Paris, 1947\n",
      "\n",
      "1870\n",
      "\n",
      "1880\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "1890\n",
      "\n",
      "18\n",
      "HN Response: The capital of Paris is \n",
      "A\n",
      "E\n",
      "E\n",
      "E\n",
      "=E\n",
      "\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "E\n",
      "5E\n",
      "E\n",
      "5E\n",
      "5E\n",
      "\n",
      "E\n",
      "E\n",
      "3E\n",
      "E\n",
      "3E\n",
      "E\n",
      "3E\n",
      "6E\n",
      "6E\n",
      "7E\n",
      "6E\n",
      "\n",
      "E\n",
      "6E\n",
      "3E\n",
      "3E\n",
      "7E\n",
      "3E\n",
      "6E\n",
      "\n",
      "E\n",
      "3E\n",
      "6E\n",
      "6E\n",
      "E\n",
      "E\n",
      "6E\n",
      "E\n",
      "6E\n",
      "E\n",
      "3E\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of Paris is \"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "\n",
    "\n",
    "model_response = generate(model, tokenizer, prompt, max_new_tokens=128, do_sample=True, temperature=0.7)\n",
    "print(f\"Model Response: {model_response}\")\n",
    "hn_response = generate(hypernet, tokenizer, prompt, max_new_tokens=128, do_sample=True, temperature=0.7)\n",
    "print(f\"HN Response: {hn_response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
