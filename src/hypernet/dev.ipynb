{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81354538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvkapur/anaconda3/envs/taskweaver/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Dict, Tuple, Literal, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from einops import einsum, reduce\n",
    "from functools import partial\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from operator import attrgetter\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3d862a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4a3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = AutoModelForCausalLM.from_pretrained('EleutherAI/pythia-70M-deduped')\n",
    "tokenizer = AutoTokenizer.from_pretrained('EleutherAI/pythia-70M-deduped')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33919239",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f69c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    results = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'labels': [],\n",
    "        'prompt_length': []\n",
    "    }\n",
    "\n",
    "    for prompt, completion in zip(examples['question'], examples['answer']):\n",
    "        # Tokenize prompt and completion separately\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)\n",
    "        completion_tokens = tokenizer(completion, add_special_tokens=False)  # Don't add special tokens again!\n",
    "        \n",
    "        # Concatenate the token IDs\n",
    "        input_ids = prompt_tokens['input_ids'] + completion_tokens['input_ids']\n",
    "        attention_mask = prompt_tokens['attention_mask'] + completion_tokens['attention_mask']\n",
    "        \n",
    "        # Create labels: mask prompt, keep completion\n",
    "        prompt_length = len(prompt_tokens['input_ids'])\n",
    "        labels = [-100] * prompt_length + completion_tokens['input_ids']\n",
    "        \n",
    "        results['input_ids'].append(input_ids)\n",
    "        results['attention_mask'].append(attention_mask)\n",
    "        results['labels'].append(labels)\n",
    "        results['prompt_length'].append(prompt_length)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2837597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPromptLengths(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator that handles padding and prompt lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract prompt_lengths before processing\n",
    "        prompt_lengths = None\n",
    "        if features and 'prompt_length' in features[0]:\n",
    "            prompt_lengths = [f.pop('prompt_length') for f in features]\n",
    "        \n",
    "        # Manual padding since parent class is failing\n",
    "        batch = {}\n",
    "        \n",
    "        # Get max length in batch\n",
    "        max_length = max(len(f['input_ids']) for f in features)\n",
    "        \n",
    "        # Pad each sequence\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Convert to list if needed\n",
    "            input_id = feature['input_ids']\n",
    "            if isinstance(input_id, torch.Tensor):\n",
    "                input_id = input_id.tolist()\n",
    "            \n",
    "            attn_mask = feature['attention_mask']\n",
    "            if isinstance(attn_mask, torch.Tensor):\n",
    "                attn_mask = attn_mask.tolist()\n",
    "            \n",
    "            label = feature['labels']\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.tolist()\n",
    "            \n",
    "            # Calculate padding length\n",
    "            padding_length = max_length - len(input_id)\n",
    "            \n",
    "            # Pad sequences (padding on the right for causal LM)\n",
    "            input_ids.append(input_id + [self.tokenizer.pad_token_id] * padding_length)\n",
    "            attention_mask.append(attn_mask + [0] * padding_length)\n",
    "            labels.append(label + [-100] * padding_length)  # -100 is ignored in loss\n",
    "        \n",
    "        # Convert to tensors\n",
    "        batch['input_ids'] = torch.tensor(input_ids, dtype=torch.long)\n",
    "        batch['attention_mask'] = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Add prompt_lengths back\n",
    "        if prompt_lengths is not None:\n",
    "            batch['prompt_lengths'] = torch.tensor(prompt_lengths, dtype=torch.long)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d754eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 14946 examples [00:01, 4769.20 examples/s]        \n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc='Tokenizing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeec1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLoraLinear(nn.Linear):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            lora_rank: int,\n",
    "            lora_alpha: int,\n",
    "            lora_dropout: float = 0.0,\n",
    "            bias: bool = True,\n",
    "            device=None,\n",
    "            dtype=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        assert lora_rank > 0, \"Use nn.Linear for Non-Lora Layer\"\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.lora_scaling = lora_alpha/lora_rank\n",
    "         \n",
    "\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def set_lora_paramters(self, A: torch.Tensor, B: torch.Tensor) -> None:\n",
    "        self.A = A # [batch_size x rank x input_dim]\n",
    "        self.B = B # [batch_size x output_dim x rank]\n",
    "\n",
    "    def unset_lora_parameters(self) -> None:\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # input: [batch_size x seq_len x input_dim]\n",
    "        \n",
    "        if self.A is None:\n",
    "            return F.linear(input, self.weight, self.bias)\n",
    "        \n",
    "        # Sanity check\n",
    "        batch_size = input.size(0)\n",
    "        if self.A.size(0) != batch_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size mismatch! Input batch_size={batch_size}, \"\n",
    "                f\"but LoRA A has batch_size={self.A.size(0)}. \"\n",
    "                f\"Old LoRA weights are being reused!\"\n",
    "            )\n",
    "\n",
    "        out_base = F.linear(input, self.weight, None)\n",
    "        out_delta = einsum(self.A, self.B, input, 'b r i, b o r, b s i -> b s o') # Instance-Level LoRA\n",
    "        \n",
    "        out =  out_base + self.lora_scaling * out_delta\n",
    "        if self.bias is not None:\n",
    "            out += self.bias    \n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        out = nn.Linear.extra_repr(self)\n",
    "        out += f', lora_rank={self.lora_rank}, lora_scaling={self.lora_scaling}, lora_dropout={self.lora_dropout}'\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9663cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskWeaver(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lm: AutoModelForCausalLM,\n",
    "            hidden_dim: int,\n",
    "            lora_rank: int,\n",
    "            lora_target_layers: List[str],\n",
    "            lora_alpha: float,\n",
    "            lora_dropout: float=0.0,\n",
    "            layers_module_name: str = 'layers'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.lora_target_layers = lora_target_layers\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # LLM config vals\n",
    "        self.lm_num_layers = self.lm.config.num_hidden_layers\n",
    "        self.lm_hidden_dim = self.lm.config.hidden_size\n",
    "\n",
    "\n",
    "        self.lm_layers_ref = self.get_layers_ref(layers_module_name)\n",
    "        assert isinstance(self.lm_layers_ref, nn.ModuleList), \"Layers must be an nn.ModuleList\"\n",
    "\n",
    "        dynamic_lora_fn = partial(DynamicLoraLinear, lora_rank=lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout, device=self.lm.device)\n",
    "        \n",
    "        self.module_references, self.in_features, self.out_features = self.replace_linears(self.lora_target_layers, self.lm_layers_ref, dynamic_lora_fn)\n",
    "        \n",
    "        self.semantic_proj = nn.Linear(self.lm_hidden_dim, hidden_dim)\n",
    "\n",
    "        self.module_embedding = nn.Embedding(len(lora_target_layers), hidden_dim)\n",
    "        self.matrix_embedding = nn.Embedding(2, hidden_dim)\n",
    "        self.layer_embedding = nn.Embedding(self.lm_num_layers, hidden_dim)\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            module_name: nn.ModuleDict({\n",
    "                'A': nn.Linear(hidden_dim, self.in_features[module_name] * self.lora_rank),\n",
    "                'B': nn.Linear(hidden_dim, self.out_features[module_name] * self.lora_rank)\n",
    "            }) for module_name in self.lora_target_layers\n",
    "        })\n",
    "\n",
    "    def get_layers_ref(self, layers_module_name:str) -> nn.Module:\n",
    "\n",
    "        for name, _ in self.lm.named_modules():\n",
    "            if not name or name.count('.') == 0:\n",
    "                continue\n",
    "            path, attribute = name.rsplit(\".\", 1)\n",
    "            if attribute == layers_module_name:\n",
    "                return attrgetter(name)(self.lm)\n",
    "\n",
    "\n",
    "    def replace_linears(self, lora_target_layers: List[str], lm_layers_ref:nn.ModuleList, dynamic_lora_fn:callable) -> Tuple[List[Dict[str, DynamicLoraLinear]], Dict[str, int], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Replaces target Linear layers with DynamicLoraLinears and return references, and module shapes\n",
    "\n",
    "        Args:\n",
    "            lora_target_layers (List[str])\n",
    "        \"\"\"\n",
    "\n",
    "        references = [{} for _ in range(self.lm_num_layers)]\n",
    "        in_features = {}\n",
    "        out_features = {}\n",
    "\n",
    "        for i, layer in enumerate(lm_layers_ref):\n",
    "            \n",
    "            for name, _ in layer.named_modules():\n",
    "                if not name or name.count('.') == 0:\n",
    "                    continue\n",
    "                \n",
    "                path, attribute = name.rsplit('.', 1)\n",
    "                if attribute not in lora_target_layers:\n",
    "                    continue\n",
    "                \n",
    "                parent_ref = attrgetter(path)(layer)\n",
    "                linear_ref = getattr(parent_ref, attribute)\n",
    "                assert isinstance(linear_ref, nn.Linear), \"Can only adapt nn.Linear layers\"\n",
    "                in_features[attribute] = linear_ref.in_features\n",
    "                out_features[attribute] = linear_ref.out_features\n",
    "                setattr(parent_ref, attribute, dynamic_lora_fn(in_features=linear_ref.in_features, out_features=linear_ref.out_features))\n",
    "                references[i][attribute] = getattr(parent_ref, attribute)\n",
    "\n",
    "        \n",
    "        return references, in_features, out_features\n",
    "\n",
    "    def _hypernet_forward(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor,\n",
    "            prompt_lengths: Optional[torch.Tensor] = None\n",
    "    ) -> List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]:\n",
    "\n",
    "\n",
    "        self.clear_lora_weights()        \n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        if prompt_lengths is not None:\n",
    "            seq_len = attention_mask.shape[1]\n",
    "            positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) # [1, seq_len]\n",
    "            prompt_lengths_expanded = prompt_lengths.unsqueeze(1) # [batch_size, 1]\n",
    "            prompt_mask = (positions < prompt_lengths_expanded).long()\n",
    "        else:\n",
    "            prompt_mask = attention_mask\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.lm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "            if prompt_lengths is not None:\n",
    "                last_prompt_indices = prompt_lengths - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_prompt_indices\n",
    "                ] # [batch, hidden]\n",
    "            else:\n",
    "                last_indices = attention_mask.sum(dim=1) - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_indices\n",
    "                ]\n",
    "    \n",
    "        semantic_embedding = self.semantic_proj(semantic_embedding.detach())\n",
    "\n",
    "        lora_weights = []\n",
    "\n",
    "        for layer_idx in range(self.lm_num_layers):\n",
    "            \n",
    "            layer_dict = {}\n",
    "            layer_emb = self.layer_embedding.weight[layer_idx:layer_idx+1]\n",
    "\n",
    "            for module_idx, module_name in enumerate(self.lora_target_layers):\n",
    "\n",
    "                module_dict = {}\n",
    "                module_emb = self.module_embedding.weight[module_idx:module_idx+1]\n",
    "\n",
    "                for matrix_idx, matrix_name in enumerate(['A', 'B']):\n",
    "\n",
    "                    matrix_emb = self.matrix_embedding.weight[matrix_idx:matrix_idx+1]\n",
    "\n",
    "                    combined_emb = semantic_embedding + layer_emb + module_emb + matrix_emb\n",
    "                    combined_emb = self.mlp(combined_emb)\n",
    "                    flat_weight = self.heads[module_name][matrix_name](combined_emb)\n",
    "\n",
    "                    if matrix_name == 'A':\n",
    "                        weight = flat_weight.view(batch_size, self.lora_rank, self.in_features[module_name])\n",
    "                    else:\n",
    "                        weight = flat_weight.view(batch_size, self.out_features[module_name], self.lora_rank)\n",
    "\n",
    "                    module_dict[matrix_name] = weight\n",
    "\n",
    "                layer_dict[module_name] = module_dict\n",
    "            \n",
    "            lora_weights.append(layer_dict)\n",
    "\n",
    "        return lora_weights\n",
    "    \n",
    "    \n",
    "    def inject_lora_weights(self, lora_weights: List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]) -> None:\n",
    "\n",
    "        for i, layer_dict in enumerate(self.module_references):\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].set_lora_paramters(**lora_weights[i][module_name])\n",
    "\n",
    "    def clear_lora_weights(self) -> None:\n",
    "        for layer_dict in self.module_references:\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].unset_lora_parameters()\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids:torch.Tensor, \n",
    "            attention_mask:torch.Tensor, \n",
    "            labels:Optional[torch.Tensor]=None, \n",
    "            prompt_lengths:Optional[torch.Tensor]=None,\n",
    "            skip_hypernet: bool = False\n",
    "        ):\n",
    "        \n",
    "        if not skip_hypernet:\n",
    "            lora_weights = self._hypernet_forward(input_ids=input_ids, attention_mask=attention_mask, prompt_lengths=prompt_lengths)\n",
    "            self.inject_lora_weights(lora_weights)\n",
    "        outputs = self.lm(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.lm.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbf977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = TaskWeaver(\n",
    "    lm,\n",
    "    hidden_dim=1024,\n",
    "    lora_rank=2, \n",
    "    lora_target_layers=['query_key_value'],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44909484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPromptLengths(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./taskweaver_output',\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=10000,\n",
    "    fp16=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hypernet,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb90e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188' max='1404' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 188/1404 01:49 < 11:53, 1.70 it/s, Epoch 0.40/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>32.124100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>22.556000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>13.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>10.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>7.592100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7.020600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>16.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>7.574400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.969500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>6.673100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>6.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>6.634100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>6.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>6.472300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>6.502900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ec597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskweaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
