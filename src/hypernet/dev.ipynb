{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81354538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvkapur/anaconda3/envs/taskweaver/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Dict, Tuple, Literal, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from einops import einsum, reduce\n",
    "from functools import partial\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from operator import attrgetter\n",
    "\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be3d862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# login(token=os.environ['HF_TOKEN'])\n",
    "\n",
    "# model = 'EleutherAI/pythia-70M-deduped'\n",
    "model = 'google/gemma-3-270m-it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b4a3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = AutoModelForCausalLM.from_pretrained(model, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33919239",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\n",
      "Answer: Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\n",
      "Working 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\n",
      "#### 10\n",
      "LM Out: earn?\n",
      "\n",
      "A. 12 dollars\n",
      "B. 15 dollars\n",
      "C. 20 dollars\n",
      "D. 25 dollars\n",
      "\n",
      "**Answer:** The correct answer is B. 15 dollars.\n",
      "<end_of_turn>\n"
     ]
    }
   ],
   "source": [
    "question = dataset['train'][1]['question']\n",
    "answer = dataset['train'][1]['answer']\n",
    "\n",
    "inputs = tokenizer(question, return_tensors='pt')\n",
    "inputs = {k:v.to('mps') for k,v in inputs.items()}\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "\n",
    "lm_out = lm.generate(**inputs, max_new_tokens=128, temperature=1.0).squeeze()\n",
    "print(f\"LM Out: {tokenizer.decode(lm_out)[len(question):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f69c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    results = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'labels': [],\n",
    "        'prompt_length': []\n",
    "    }\n",
    "\n",
    "    for prompt, completion in zip(examples['question'], examples['answer']):\n",
    "        # Tokenize prompt and completion separately\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)\n",
    "        completion_tokens = tokenizer(completion, add_special_tokens=False)  # Don't add special tokens again!\n",
    "        \n",
    "        # Concatenate the token IDs\n",
    "        input_ids = prompt_tokens['input_ids'] + completion_tokens['input_ids']\n",
    "        attention_mask = prompt_tokens['attention_mask'] + completion_tokens['attention_mask']\n",
    "        \n",
    "        # Create labels: mask prompt, keep completion\n",
    "        prompt_length = len(prompt_tokens['input_ids'])\n",
    "        labels = [-100] * prompt_length + completion_tokens['input_ids']\n",
    "        \n",
    "        results['input_ids'].append(input_ids)\n",
    "        results['attention_mask'].append(attention_mask)\n",
    "        results['labels'].append(labels)\n",
    "        results['prompt_length'].append(prompt_length)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2837597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPromptLengths(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator that handles padding and prompt lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract prompt_lengths before processing\n",
    "        prompt_lengths = None\n",
    "        if features and 'prompt_length' in features[0]:\n",
    "            prompt_lengths = [f.pop('prompt_length') for f in features]\n",
    "        \n",
    "        # Manual padding since parent class is failing\n",
    "        batch = {}\n",
    "        \n",
    "        # Get max length in batch\n",
    "        max_length = max(len(f['input_ids']) for f in features)\n",
    "        \n",
    "        # Pad each sequence\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Convert to list if needed\n",
    "            input_id = feature['input_ids']\n",
    "            if isinstance(input_id, torch.Tensor):\n",
    "                input_id = input_id.tolist()\n",
    "            \n",
    "            attn_mask = feature['attention_mask']\n",
    "            if isinstance(attn_mask, torch.Tensor):\n",
    "                attn_mask = attn_mask.tolist()\n",
    "            \n",
    "            label = feature['labels']\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.tolist()\n",
    "            \n",
    "            # Calculate padding length\n",
    "            padding_length = max_length - len(input_id)\n",
    "            \n",
    "            # Pad sequences (padding on the right for causal LM)\n",
    "            input_ids.append(input_id + [self.tokenizer.pad_token_id] * padding_length)\n",
    "            attention_mask.append(attn_mask + [0] * padding_length)\n",
    "            labels.append(label + [-100] * padding_length)  # -100 is ignored in loss\n",
    "        \n",
    "        # Convert to tensors\n",
    "        batch['input_ids'] = torch.tensor(input_ids, dtype=torch.long)\n",
    "        batch['attention_mask'] = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Add prompt_lengths back\n",
    "        if prompt_lengths is not None:\n",
    "            batch['prompt_lengths'] = torch.tensor(prompt_lengths, dtype=torch.long)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d754eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 14946 examples [00:01, 4991.29 examples/s]        \n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc='Tokenizing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeec1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLoraLinear(nn.Linear):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            lora_rank: int,\n",
    "            lora_alpha: int,\n",
    "            lora_dropout: float = 0.0,\n",
    "            bias: bool = True,\n",
    "            device=None,\n",
    "            dtype=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        assert lora_rank > 0, \"Use nn.Linear for Non-Lora Layer\"\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.lora_scaling = lora_alpha/lora_rank\n",
    "         \n",
    "\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def set_lora_paramters(self, A: torch.Tensor, B: torch.Tensor) -> None:\n",
    "        self.A = A # [batch_size x rank x input_dim]\n",
    "        self.B = B # [batch_size x output_dim x rank]\n",
    "\n",
    "    def unset_lora_parameters(self) -> None:\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # input: [batch_size x seq_len x input_dim]\n",
    "        \n",
    "        if self.A is None:\n",
    "            return F.linear(input, self.weight, self.bias)\n",
    "        \n",
    "        # Sanity check\n",
    "        batch_size = input.size(0)\n",
    "        if self.A.size(0) != batch_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size mismatch! Input batch_size={batch_size}, \"\n",
    "                f\"but LoRA A has batch_size={self.A.size(0)}. \"\n",
    "                f\"Old LoRA weights are being reused!\"\n",
    "            )\n",
    "\n",
    "        out_base = F.linear(input, self.weight, None)\n",
    "        out_delta = einsum(self.A, self.B, input, 'b r i, b o r, b s i -> b s o') # Instance-Level LoRA\n",
    "        \n",
    "        out =  out_base + self.lora_scaling * out_delta\n",
    "        if self.bias is not None:\n",
    "            out += self.bias    \n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        out = nn.Linear.extra_repr(self)\n",
    "        out += f', lora_rank={self.lora_rank}, lora_scaling={self.lora_scaling}, lora_dropout={self.lora_dropout}'\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9663cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskWeaver(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lm: AutoModelForCausalLM,\n",
    "            hidden_dim: int,\n",
    "            lora_rank: int,\n",
    "            lora_target_layers: List[str],\n",
    "            lora_alpha: float,\n",
    "            lora_dropout: float=0.0,\n",
    "            layers_module_name: str = 'layers'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.lora_target_layers = lora_target_layers\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # LLM config vals\n",
    "        self.lm_num_layers = self.lm.config.num_hidden_layers\n",
    "        self.lm_hidden_dim = self.lm.config.hidden_size\n",
    "\n",
    "\n",
    "        lm_layers_ref = self.get_layers_ref(layers_module_name)\n",
    "        assert isinstance(lm_layers_ref, nn.ModuleList), \"Layers must be an nn.ModuleList\"\n",
    "\n",
    "        dynamic_lora_fn = partial(DynamicLoraLinear, lora_rank=lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout, device=self.lm.device)\n",
    "        \n",
    "        self.module_references, self.in_features, self.out_features = self.replace_linears(self.lora_target_layers, lm_layers_ref, dynamic_lora_fn)\n",
    "        \n",
    "        self.semantic_proj = nn.Linear(self.lm_hidden_dim, hidden_dim)\n",
    "\n",
    "        self.module_embedding = nn.Embedding(len(lora_target_layers), hidden_dim)\n",
    "        self.matrix_embedding = nn.Embedding(2, hidden_dim)\n",
    "        self.layer_embedding = nn.Embedding(self.lm_num_layers, hidden_dim)\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            module_name: nn.ModuleDict({\n",
    "                'A': nn.Linear(hidden_dim, self.in_features[module_name] * self.lora_rank),\n",
    "                'B': nn.Linear(hidden_dim, self.out_features[module_name] * self.lora_rank)\n",
    "            }) for module_name in self.lora_target_layers\n",
    "        })\n",
    "\n",
    "        self._init_weights()\n",
    "        self._freeze_lm()\n",
    "\n",
    "    def _freeze_lm(self):\n",
    "\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize MLP layers with smaller weights\n",
    "        for module in [self.semantic_proj, self.mlp]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "        \n",
    "        # Initialize output heads to produce small initial LoRA weights\n",
    "        for module_name in self.lora_target_layers:\n",
    "            for matrix_name in ['A', 'B']:\n",
    "                head = self.heads[module_name][matrix_name]\n",
    "                nn.init.zeros_(head.weight)  # Start with zero weights\n",
    "                \n",
    "                if matrix_name == 'A':\n",
    "                    # Small random bias for A matrix\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.uniform_(head.bias, -1/np.sqrt(self.in_features[module_name]), \n",
    "                                                    1/np.sqrt(self.in_features[module_name]))\n",
    "                else:  # B matrix\n",
    "                    # Zero bias for B matrix (standard LoRA init)\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.zeros_(head.bias)\n",
    "\n",
    "    def get_layers_ref(self, layers_module_name:str) -> nn.Module:\n",
    "\n",
    "        for name, _ in self.lm.named_modules():\n",
    "            if not name or name.count('.') == 0:\n",
    "                continue\n",
    "            path, attribute = name.rsplit(\".\", 1)\n",
    "            if attribute == layers_module_name:\n",
    "                return attrgetter(name)(self.lm)\n",
    "\n",
    "\n",
    "    def replace_linears(self, lora_target_layers: List[str], lm_layers_ref:nn.ModuleList, dynamic_lora_fn:callable) -> Tuple[List[Dict[str, DynamicLoraLinear]], Dict[str, int], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Replaces target Linear layers with DynamicLoraLinears and return references, and module shapes\n",
    "\n",
    "        Args:\n",
    "            lora_target_layers (List[str])\n",
    "        \"\"\"\n",
    "\n",
    "        references = [{} for _ in range(self.lm_num_layers)]\n",
    "        in_features = {}\n",
    "        out_features = {}\n",
    "\n",
    "        for i, layer in enumerate(lm_layers_ref):\n",
    "            \n",
    "            for name, _ in layer.named_modules():\n",
    "                if not name or name.count('.') == 0:\n",
    "                    continue\n",
    "                \n",
    "                path, attribute = name.rsplit('.', 1)\n",
    "                if attribute not in lora_target_layers:\n",
    "                    continue\n",
    "                \n",
    "                parent_ref = attrgetter(path)(layer)\n",
    "                linear_ref = getattr(parent_ref, attribute)\n",
    "                assert isinstance(linear_ref, nn.Linear), \"Can only adapt nn.Linear layers\"\n",
    "                in_features[attribute] = linear_ref.in_features\n",
    "                out_features[attribute] = linear_ref.out_features\n",
    "                setattr(parent_ref, attribute, dynamic_lora_fn(in_features=linear_ref.in_features, out_features=linear_ref.out_features))\n",
    "                references[i][attribute] = getattr(parent_ref, attribute)\n",
    "\n",
    "        \n",
    "        return references, in_features, out_features\n",
    "\n",
    "    def _hypernet_forward(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor,\n",
    "            prompt_lengths: Optional[torch.Tensor] = None\n",
    "    ) -> List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]:\n",
    "\n",
    "\n",
    "        self.clear_lora_weights()        \n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        if prompt_lengths is not None:\n",
    "            seq_len = attention_mask.shape[1]\n",
    "            positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) # [1, seq_len]\n",
    "            prompt_lengths_expanded = prompt_lengths.unsqueeze(1) # [batch_size, 1]\n",
    "            prompt_mask = (positions < prompt_lengths_expanded).long()\n",
    "        else:\n",
    "            prompt_mask = attention_mask\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.lm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "            if prompt_lengths is not None:\n",
    "                last_prompt_indices = prompt_lengths - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_prompt_indices\n",
    "                ] # [batch, hidden]\n",
    "            else:\n",
    "                last_indices = attention_mask.sum(dim=1) - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_indices\n",
    "                ]\n",
    "    \n",
    "        semantic_embedding = self.semantic_proj(semantic_embedding.detach())\n",
    "\n",
    "        lora_weights = []\n",
    "\n",
    "        for layer_idx in range(self.lm_num_layers):\n",
    "            \n",
    "            layer_dict = {}\n",
    "            layer_emb = self.layer_embedding.weight[layer_idx:layer_idx+1]\n",
    "\n",
    "            for module_idx, module_name in enumerate(self.lora_target_layers):\n",
    "\n",
    "                module_dict = {}\n",
    "                module_emb = self.module_embedding.weight[module_idx:module_idx+1]\n",
    "\n",
    "                for matrix_idx, matrix_name in enumerate(['A', 'B']):\n",
    "\n",
    "                    matrix_emb = self.matrix_embedding.weight[matrix_idx:matrix_idx+1]\n",
    "\n",
    "                    combined_emb = semantic_embedding + layer_emb + module_emb + matrix_emb\n",
    "                    combined_emb = self.mlp(combined_emb)\n",
    "                    flat_weight = self.heads[module_name][matrix_name](combined_emb)\n",
    "\n",
    "                    if matrix_name == 'A':\n",
    "                        weight = flat_weight.view(batch_size, self.lora_rank, self.in_features[module_name])\n",
    "                    else:\n",
    "                        weight = flat_weight.view(batch_size, self.out_features[module_name], self.lora_rank)\n",
    "                    \n",
    "                    module_dict[matrix_name] = weight\n",
    "\n",
    "                layer_dict[module_name] = module_dict\n",
    "            \n",
    "            lora_weights.append(layer_dict)\n",
    "\n",
    "        return lora_weights\n",
    "    \n",
    "    \n",
    "    def inject_lora_weights(self, lora_weights: List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]) -> None:\n",
    "\n",
    "        for i, layer_dict in enumerate(self.module_references):\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].set_lora_paramters(**lora_weights[i][module_name])\n",
    "\n",
    "    def clear_lora_weights(self) -> None:\n",
    "        for layer_dict in self.module_references:\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].unset_lora_parameters()\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids:torch.Tensor, \n",
    "            attention_mask:torch.Tensor, \n",
    "            labels:Optional[torch.Tensor]=None, \n",
    "            prompt_lengths:Optional[torch.Tensor]=None,\n",
    "            skip_hypernet: bool = False\n",
    "        ):\n",
    "        \n",
    "        if not skip_hypernet:\n",
    "            lora_weights = self._hypernet_forward(input_ids=input_ids, attention_mask=attention_mask, prompt_lengths=prompt_lengths)\n",
    "            self.inject_lora_weights(lora_weights)\n",
    "        outputs = self.lm(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        prompt_lengths: Optional[torch.Tensor] = None,\n",
    "        **generation_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate text using the task-adapted model.\n",
    "\n",
    "        This method first generates LoRA weights using the hypernetwork,\n",
    "        injects them into the model, and then runs generation.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask (optional)\n",
    "            prompt_lengths: Length of prompts in each sequence (optional)\n",
    "            **generation_kwargs: Additional arguments passed to the LM's generate method\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Generate LoRA weights based on the prompt\n",
    "        lora_weights = self._hypernet_forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            prompt_lengths=prompt_lengths\n",
    "        )\n",
    "\n",
    "        # Inject LoRA weights into the model\n",
    "        self.inject_lora_weights(lora_weights)\n",
    "\n",
    "        # Generate using the adapted model\n",
    "        try:\n",
    "            outputs = self.lm.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        finally:\n",
    "            # Clear LoRA weights after generation\n",
    "            self.clear_lora_weights()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.lm.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbf977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = TaskWeaver(\n",
    "    lm,\n",
    "    hidden_dim=256,\n",
    "    lora_rank=2, \n",
    "    lora_target_layers=['query_key_value'],\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0854fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71745536\n",
      "1318912\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in hypernet.parameters())\n",
    "trainable_params = sum(p.numel() for p in hypernet.parameters() if p.requires_grad)\n",
    "\n",
    "print(total_params)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44909484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPromptLengths(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./taskweaver_output',\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hypernet,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb90e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 56/468 00:31 < 03:56, 1.74 it/s, Epoch 0.12/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>7.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.037100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.569600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.370900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.298000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 228\u001b[39m, in \u001b[36mTaskWeaver.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, labels, prompt_lengths, skip_hypernet)\u001b[39m\n\u001b[32m    226\u001b[39m     lora_weights = \u001b[38;5;28mself\u001b[39m._hypernet_forward(input_ids=input_ids, attention_mask=attention_mask, prompt_lengths=prompt_lengths)\n\u001b[32m    227\u001b[39m     \u001b[38;5;28mself\u001b[39m.inject_lora_weights(lora_weights)\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/utils/generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:577\u001b[39m, in \u001b[36mGPTNeoXForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    552\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    553\u001b[39m ) -> Union[\u001b[38;5;28mtuple\u001b[39m, CausalLMOutputWithPast]:\n\u001b[32m    554\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    555\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m    556\u001b[39m \u001b[33;03m        Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    574\u001b[39m \u001b[33;03m    >>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    592\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/utils/generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:443\u001b[39m, in \u001b[36mGPTNeoXModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, cache_position, **kwargs)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    441\u001b[39m     position_ids = cache_position.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m causal_mask = \u001b[43mcreate_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m    454\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m    455\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m    457\u001b[39m converted_head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/masking_utils.py:825\u001b[39m, in \u001b[36mcreate_causal_mask\u001b[39m\u001b[34m(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, or_mask_function, and_mask_function)\u001b[39m\n\u001b[32m    822\u001b[39m     allow_is_causal_skip = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[38;5;66;03m# We now create the mask\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m causal_mask = \u001b[43mmask_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    828\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    829\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    830\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_function\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_factory_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    832\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_is_causal_skip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# additional kwarg for sdpa\u001b[39;49;00m\n\u001b[32m    833\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Additional kwarg for eager\u001b[39;49;00m\n\u001b[32m    834\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Pass the config as well, in case someone wants to easily have their own mask_interface\u001b[39;49;00m\n\u001b[32m    835\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m causal_mask\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/taskweaver/lib/python3.12/site-packages/transformers/masking_utils.py:379\u001b[39m, in \u001b[36msdpa_mask_recent_torch\u001b[39m\u001b[34m(batch_size, cache_position, kv_length, kv_offset, mask_function, attention_mask, local_size, allow_is_causal_skip, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\u001b[39;00m\n\u001b[32m    378\u001b[39m \u001b[38;5;66;03m# but without data-dependent slicing (i.e. torch.compile friendly)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m kv_arange = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkv_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m kv_arange += kv_offset\n\u001b[32m    382\u001b[39m \u001b[38;5;66;03m# Potentially add the padding 2D mask\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ec597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM Out: \n",
      "\n",
      "The first two clips were taken from the same group, and the third one was taken from\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HN Out: The total of the number of the number of the number of the number of the number of the number\n",
      "HN LM Out:  m'''''''''''''''''''\n"
     ]
    }
   ],
   "source": [
    "hn_out = hypernet.generate(**inputs).squeeze()\n",
    "print(f\"HN Out: {tokenizer.decode(hn_out)[len(question):]}\")\n",
    "\n",
    "hn_model_out = hypernet.lm.generate(**inputs).squeeze()\n",
    "print(f\"HN LM Out: {tokenizer.decode(hn_model_out)[len(question):]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6534bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67712f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3279d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskweaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
