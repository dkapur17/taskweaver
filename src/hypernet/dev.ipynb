{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5aaa16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvkapur/anaconda3/envs/taskweaver/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer.sft_trainer import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Dict, Tuple, Literal, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from einops import einsum, reduce\n",
    "from functools import partial\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from operator import attrgetter\n",
    "from dsconf import DatasetConfig\n",
    "\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4119ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'EleutherAI/pythia-70m'\n",
    "# model = 'google/gemma-3-270m'\n",
    "# model = 'google/gemma-3-270m-it'\n",
    "# model = 'Qwen/Qwen3-0.6B'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51946a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = AutoModelForCausalLM.from_pretrained(model)\n",
    "lm = AutoModelForCausalLM.from_pretrained(model, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "is_chat = tokenizer.chat_template is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8490fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 85666.12 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 50692.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_config = DatasetConfig.from_dataset_path('openai/gsm8k', 'main')\n",
    "train_dataset = dataset_config.load_and_process(is_chat, 'train', enable_thinking=False)\n",
    "eval_dataset = dataset_config.load_and_process(is_chat, 'test', enable_thinking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ef0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLoraLinear(nn.Linear):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            lora_rank: int,\n",
    "            lora_alpha: int,\n",
    "            lora_dropout: float = 0.0,\n",
    "            bias: bool = True,\n",
    "            device=None,\n",
    "            dtype=None\n",
    "    ):\n",
    "        \n",
    "        super().__init__(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        assert lora_rank > 0, \"Use nn.Linear for Non-Lora Layer\"\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.lora_scaling = lora_alpha/lora_rank\n",
    "         \n",
    "\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def set_lora_paramters(self, A: torch.Tensor, B: torch.Tensor) -> None:\n",
    "        self.A = A # [batch_size x rank x input_dim]\n",
    "        self.B = B # [batch_size x output_dim x rank]\n",
    "\n",
    "    def replicate(self, target: nn.Linear) -> None:\n",
    "        assert isinstance(target, nn.Linear), \"Can only replicate nn.Linear\"\n",
    "\n",
    "        self.weight.data = target.weight.data\n",
    "        if self.bias is not None:\n",
    "            self.bias.data = target.bias.data\n",
    "\n",
    "    def unset_lora_parameters(self) -> None:\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # input: [batch_size x seq_len x input_dim]\n",
    "        \n",
    "        if self.A is None:\n",
    "            return F.linear(input, self.weight, self.bias)\n",
    "        \n",
    "        # Sanity check\n",
    "        batch_size = input.size(0)\n",
    "        if self.A.size(0) != batch_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size mismatch! Input batch_size={batch_size}, \"\n",
    "                f\"but LoRA A has batch_size={self.A.size(0)}. \"\n",
    "                f\"Old LoRA weights are being reused!\"\n",
    "            )\n",
    "\n",
    "        out_base = F.linear(input, self.weight, None)\n",
    "        out_delta = einsum(self.A, self.B, input, 'b r i, b o r, b s i -> b s o') # Instance-Level LoRA\n",
    "        \n",
    "        out =  out_base + self.lora_scaling * out_delta\n",
    "        if self.bias is not None:\n",
    "            out += self.bias    \n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        out = nn.Linear.extra_repr(self)\n",
    "        out += f', lora_rank={self.lora_rank}, lora_scaling={self.lora_scaling}, lora_dropout={self.lora_dropout}'\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d00dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskWeaver(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lm: AutoModelForCausalLM,\n",
    "            hidden_dim: int,\n",
    "            lora_rank: int,\n",
    "            lora_target_layers: List[str],\n",
    "            lora_alpha: float,\n",
    "            lora_dropout: float=0.0,\n",
    "            layers_module_name: str = 'layers'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.lora_target_layers = lora_target_layers\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # LLM config vals\n",
    "        self.lm_num_layers = self.lm.config.num_hidden_layers\n",
    "        self.lm_hidden_dim = self.lm.config.hidden_size\n",
    "\n",
    "\n",
    "        lm_layers_ref = self.get_layers_ref(layers_module_name)\n",
    "        assert isinstance(lm_layers_ref, nn.ModuleList), \"Layers must be an nn.ModuleList\"\n",
    "\n",
    "        dynamic_lora_fn = partial(DynamicLoraLinear, lora_rank=lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout, device=self.lm.device)\n",
    "        \n",
    "        self.module_references, self.in_features, self.out_features = self.replace_linears(self.lora_target_layers, lm_layers_ref, dynamic_lora_fn)\n",
    "        \n",
    "        self.semantic_proj = nn.Linear(self.lm_hidden_dim, hidden_dim)\n",
    "\n",
    "        self.module_embedding = nn.Embedding(len(lora_target_layers), hidden_dim)\n",
    "        self.matrix_embedding = nn.Embedding(2, hidden_dim)\n",
    "        self.layer_embedding = nn.Embedding(self.lm_num_layers, hidden_dim)\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            module_name: nn.ModuleDict({\n",
    "                'A': nn.Linear(hidden_dim, self.in_features[module_name] * self.lora_rank),\n",
    "                'B': nn.Linear(hidden_dim, self.out_features[module_name] * self.lora_rank)\n",
    "            }) for module_name in self.lora_target_layers\n",
    "        })\n",
    "\n",
    "        self._init_weights()\n",
    "        self._freeze_lm()\n",
    "\n",
    "    def _freeze_lm(self):\n",
    "\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize MLP layers with smaller weights\n",
    "        for module in [self.semantic_proj, self.mlp]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "        \n",
    "        # Initialize output heads to produce small initial LoRA weights\n",
    "        for module_name in self.lora_target_layers:\n",
    "            for matrix_name in ['A', 'B']:\n",
    "                head = self.heads[module_name][matrix_name]\n",
    "                nn.init.zeros_(head.weight)  # Start with zero weights\n",
    "                \n",
    "                if matrix_name == 'A':\n",
    "                    # Small random bias for A matrix\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.uniform_(head.bias, -1/np.sqrt(self.in_features[module_name]), \n",
    "                                                    1/np.sqrt(self.in_features[module_name]))\n",
    "                else:  # B matrix\n",
    "                    # Zero bias for B matrix (standard LoRA init)\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.zeros_(head.bias)\n",
    "\n",
    "    def get_layers_ref(self, layers_module_name:str) -> nn.Module:\n",
    "\n",
    "        for name, _ in self.lm.named_modules():\n",
    "            if not name or name.count('.') == 0:\n",
    "                continue\n",
    "            path, attribute = name.rsplit(\".\", 1)\n",
    "            if attribute == layers_module_name:\n",
    "                return attrgetter(name)(self.lm)\n",
    "\n",
    "\n",
    "    def replace_linears(self, lora_target_layers: List[str], lm_layers_ref:nn.ModuleList, dynamic_lora_fn:callable) -> Tuple[List[Dict[str, DynamicLoraLinear]], Dict[str, int], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Replaces target Linear layers with DynamicLoraLinears and return references, and module shapes\n",
    "\n",
    "        Args:\n",
    "            lora_target_layers (List[str])\n",
    "        \"\"\"\n",
    "\n",
    "        references = [{} for _ in range(self.lm_num_layers)]\n",
    "        in_features = {}\n",
    "        out_features = {}\n",
    "\n",
    "        for i, layer in enumerate(lm_layers_ref):\n",
    "            \n",
    "            for name, _ in layer.named_modules():\n",
    "                if not name or name.count('.') == 0:\n",
    "                    continue\n",
    "                \n",
    "                path, attribute = name.rsplit('.', 1)\n",
    "                if attribute not in lora_target_layers:\n",
    "                    continue\n",
    "                \n",
    "                parent_ref = attrgetter(path)(layer)\n",
    "                linear_ref = getattr(parent_ref, attribute)\n",
    "                assert isinstance(linear_ref, nn.Linear), \"Can only adapt nn.Linear layers\"\n",
    "                in_features[attribute] = linear_ref.in_features\n",
    "                out_features[attribute] = linear_ref.out_features\n",
    "                dynamic_lora_layer = dynamic_lora_fn(in_features=linear_ref.in_features, out_features=linear_ref.out_features, bias=(linear_ref.bias is not None))\n",
    "                dynamic_lora_layer.replicate(linear_ref)\n",
    "                setattr(parent_ref, attribute, dynamic_lora_layer)\n",
    "                references[i][attribute] = getattr(parent_ref, attribute)\n",
    "\n",
    "        \n",
    "        return references, in_features, out_features\n",
    "\n",
    "    def _hypernet_forward(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor,\n",
    "            prompt_length: Optional[torch.Tensor] = None\n",
    "    ) -> List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]:\n",
    "\n",
    "\n",
    "        self.clear_lora_weights()        \n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        if prompt_length is not None:\n",
    "            seq_len = attention_mask.shape[1]\n",
    "            positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) # [1, seq_len]\n",
    "            prompt_length_expanded = prompt_length.unsqueeze(1) # [batch_size, 1]\n",
    "            prompt_mask = (positions < prompt_length_expanded).long()\n",
    "        else:\n",
    "            prompt_mask = attention_mask\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.lm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "            if prompt_length is not None:\n",
    "                last_prompt_indices = prompt_length - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_prompt_indices\n",
    "                ] # [batch, hidden]\n",
    "            else:\n",
    "                last_indices = attention_mask.sum(dim=1) - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_indices\n",
    "                ]\n",
    "    \n",
    "        semantic_embedding = self.semantic_proj(semantic_embedding.detach())\n",
    "\n",
    "        lora_weights = []\n",
    "\n",
    "        for layer_idx in range(self.lm_num_layers):\n",
    "            \n",
    "            layer_dict = {}\n",
    "            layer_emb = self.layer_embedding.weight[layer_idx:layer_idx+1]\n",
    "\n",
    "            for module_idx, module_name in enumerate(self.lora_target_layers):\n",
    "\n",
    "                module_dict = {}\n",
    "                module_emb = self.module_embedding.weight[module_idx:module_idx+1]\n",
    "\n",
    "                for matrix_idx, matrix_name in enumerate(['A', 'B']):\n",
    "\n",
    "                    matrix_emb = self.matrix_embedding.weight[matrix_idx:matrix_idx+1]\n",
    "\n",
    "                    combined_emb = semantic_embedding + layer_emb + module_emb + matrix_emb\n",
    "                    combined_emb = self.mlp(combined_emb)\n",
    "                    flat_weight = self.heads[module_name][matrix_name](combined_emb)\n",
    "\n",
    "                    if matrix_name == 'A':\n",
    "                        weight = flat_weight.view(batch_size, self.lora_rank, self.in_features[module_name])\n",
    "                    else:\n",
    "                        weight = flat_weight.view(batch_size, self.out_features[module_name], self.lora_rank)\n",
    "                    \n",
    "                    module_dict[matrix_name] = weight\n",
    "\n",
    "                layer_dict[module_name] = module_dict\n",
    "            \n",
    "            lora_weights.append(layer_dict)\n",
    "\n",
    "        return lora_weights\n",
    "    \n",
    "    \n",
    "    def inject_lora_weights(self, lora_weights: List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]) -> None:\n",
    "\n",
    "        for i, layer_dict in enumerate(self.module_references):\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].set_lora_paramters(**lora_weights[i][module_name])\n",
    "\n",
    "    def clear_lora_weights(self) -> None:\n",
    "        for layer_dict in self.module_references:\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].unset_lora_parameters()\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids:torch.Tensor, \n",
    "            attention_mask:torch.Tensor, \n",
    "            labels:Optional[torch.Tensor]=None, \n",
    "            prompt_length:Optional[torch.Tensor]=None,\n",
    "            skip_hypernet: bool = False,\n",
    "            **lm_kwargs\n",
    "        ):\n",
    "        \n",
    "        if not skip_hypernet:\n",
    "            lora_weights = self._hypernet_forward(input_ids=input_ids, attention_mask=attention_mask, prompt_length=prompt_length)\n",
    "            self.inject_lora_weights(lora_weights)\n",
    "        outputs = self.lm(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **lm_kwargs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        prompt_length: Optional[torch.Tensor] = None,\n",
    "        **generation_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate text using the task-adapted model.\n",
    "\n",
    "        This method first generates LoRA weights using the hypernetwork,\n",
    "        injects them into the model, and then runs generation.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask (optional)\n",
    "            prompt_length: Length of prompts in each sequence (optional)\n",
    "            **generation_kwargs: Additional arguments passed to the LM's generate method\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Generate LoRA weights based on the prompt\n",
    "        lora_weights = self._hypernet_forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            prompt_length=prompt_length\n",
    "        )\n",
    "\n",
    "        # Inject LoRA weights into the model\n",
    "        self.inject_lora_weights(lora_weights)\n",
    "\n",
    "        # Generate using the adapted model\n",
    "        try:\n",
    "            outputs = self.lm.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        finally:\n",
    "            # Clear LoRA weights after generation\n",
    "            self.clear_lora_weights()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.lm.device\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self.lm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9748cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = TaskWeaver(\n",
    "    lm,\n",
    "    hidden_dim=256,\n",
    "    lora_rank=2, \n",
    "    lora_target_layers=['query_key_value'],\n",
    "    # lora_target_layers=['q_proj', 'v_proj'],\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bffda87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71745536\n",
      "1318912\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in hypernet.parameters())\n",
    "trainable_params = sum(p.numel() for p in hypernet.parameters() if p.requires_grad)\n",
    "\n",
    "print(total_params)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f22df0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorWithPromptLenghts(DataCollatorForLanguageModeling):\n",
    "\n",
    "    def __call__(self, examples:List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "        batch['prompt_length'] = (batch['labels'] != -100).int().argmax(dim=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92855fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "\n",
    "collator = DataCollatorWithPromptLenghts(pad_token_id=pad_token_id)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    num_train_epochs = 1.0,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    bf16 = False,\n",
    "    logging_steps = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c412f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 75652.85 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:02<00:00, 2586.36 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 897981.20 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 68207.21 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 2666.57 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 431300.15 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=hypernet,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6519d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "/Users/dhruvkapur/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1869' max='1869' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1869/1869 05:41, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.694700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.547100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.471800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.540300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.293300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.796700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.769800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.812100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.626600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.705900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.612500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.418200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.447900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.221100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.299100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.383900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.320900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.430100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.244300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.212700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.379600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.329800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.428400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.049200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.152200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.230500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.373800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.198100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.185700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.207600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.230700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.171600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.176800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.179900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.233900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.159300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.127600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.110100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.035900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.096100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>1.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.054700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>1.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.059400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.098700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>1.984600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.178600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.127800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.965900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.045300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.148000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.003800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.980100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>1.990100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.945100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>1.988800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>1.979900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>1.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>1.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.016300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>1.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.070100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.886600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>1.887200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>1.995100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>1.892300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.004000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.001900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>1.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>1.999100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.878500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.018200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>1.924200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>1.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.062000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.884300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>1.847000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>1.936900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>1.949000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>1.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.833800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>1.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>1.834300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>1.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.886300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>1.926400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>1.831500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.107800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>1.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.813600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>1.932300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>1.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>1.963500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>1.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>1.866300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>1.903700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>1.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>1.865500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.822300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>1.906500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>1.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>1.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.948200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>1.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>1.883700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>1.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>1.956800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>1.793400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>1.923000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>1.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>1.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.868700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>1.889700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>1.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>1.936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>2.039100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.724500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>1.846600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>1.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>1.841400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>1.799800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.706200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>1.914700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>1.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>1.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>1.762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>1.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>1.905300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>1.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.751200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>1.861100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>1.800200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>1.893800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.647400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.681900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>1.754000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>1.707900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>1.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>1.736700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>1.753500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1869, training_loss=2.1025782595859486, metrics={'train_runtime': 341.745, 'train_samples_per_second': 21.867, 'train_steps_per_second': 5.469, 'total_flos': 0.0, 'train_loss': 2.1025782595859486, 'entropy': 2.6636132773231056, 'num_tokens': 1458952.0, 'mean_token_accuracy': 0.6013506580801571, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "365e1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(hypernet, control, tokenizer, sample, **generation_kwargs):\n",
    "    inputs = tokenizer(text=sample['prompt'], return_tensors='pt')\n",
    "    hypernet_inputs = {k:v.to(hypernet.device) for k,v in inputs.items()}\n",
    "    control_inputs = {k:v.to(control.device) for k,v in inputs.items()}\n",
    "    \n",
    "    hypernet_output_ids = hypernet.generate(**hypernet_inputs, **generation_kwargs)[0]\n",
    "    control_output_ids = control.generate(**control_inputs, **generation_kwargs)[0]\n",
    "\n",
    "    hypernet_generated_ids = hypernet_output_ids[len(hypernet_inputs['input_ids'][0]):]\n",
    "    control_generated_ids = control_output_ids[len(control_inputs['input_ids'][0]):]\n",
    "\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Prompt\\n{sample['prompt']}\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Completion\\n{sample['completion']}\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Control Answer\\n{tokenizer.decode(control_generated_ids)}\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Hypernet Answer\\n{tokenizer.decode(hypernet_generated_ids)}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d3c9c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Prompt\n",
      "Instruction: Analyze the given math problem, reason through it step by step, and provide the final answer in a new line starting with ####, for example: #### 72\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Answer:\n",
      "==============================\n",
      "Completion\n",
      " Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "==============================\n",
      "Control Answer\n",
      " I do.\n",
      "Answer: We need to put the cost of each egg every day at the farmer's market twice a week. What would go to the farmer's market every week and make the cost of the each egg every day?\n",
      "Answer: Not only does it cost the farm to produce the amount of eggs the animals feed, and there is always the cost of the each egg. That is enough for her to buy eggs for the first one, and she could have the same number, which would generate that. Now we need to cut out the size of the eggs ourselves. That is enough to keep the eggs in one go to his neighbor’s house that they have the same weight and cost during each month when buying the ducks. The egg would be placed to the farmer’s market and sell at an acre. In any case, this would be the price that would be a very good price. The goal being to make the eggs an easy, productive property of nature, so that people could buy them. It’s just as important that the cost of working and producing animals is taken down by the farmer's market. So if you find yourself wondering if he’ll be in the way of sales instead of producing his eggs, you can think of things like the following, you can imagine that there are more farms in the market than there are other farms in the market. These farms need to be profitable in order to maintain these profit margins, and the value of the plants they feed each year are the way to go.\n",
      "Answer: There is one good reason to keep the chickens in one go to their neighbors’ farms.\n",
      "Answer: There are a number of farms built by farming in the markets, and they sell the chicken eggs. These include farming with some of those and others, like the company Finder, which is based on an agricultural technique. The farm is built so that an animal could be able to harvest anything from the chickens at their home by buying eggs.\n",
      "Answer: Some farmers are more interested in farming in the market than in other farms. Others seek the farmers to create the chicken eggs to themselves. There is no need to take chickens into consideration in the market. The market and a well-run farm have different prices to ensure that each has a different price for each and every chicken. When your neighbor buys and sells eggs, his or her house will get the same market.\n",
      "Question: Is it not enough to the farm to feed the chickens/birds with them in a commercial market?\n",
      "Answer\n",
      "==============================\n",
      "Hypernet Answer\n",
      " Janet has 24 eggs for every $2/mth hour. So she has 14 eggs for the remaining 24/mth hour at the farmer's market every day – 10 eggs will be spent every Monday of course and 2 eggs will be spent each Monday of course. So she made one-1/mth hour for each day.\n",
      "As noted above, there is a total of 2/mth hour for the farm yoke because 14 eggs per day is 10 yth hour/d/meal.\n",
      "This is a total of 14 eggs per day – 11 eggs will be spent every Sunday when the second day is spent. The total should be 10 yth hour/d/hour.\n",
      "However, in general, there is a total of 2/mth hours, which means she will spend 14 eggs for the next Sunday and 10 eggs for the next Sunday at 3-2/d/meal.\n",
      "The following amounts are the amount of extra hours.\n",
      "These amounts are divided by their minimum hours (18 to 20/24 hour) and minimum hours (2/mth hour) per hour.\n",
      "So, the amount of extra hours/d= <<1/mth hour/d/meal.\n",
      "In total, there are 10/6 + 9/4 + 5/2 + 25 + 20 - 50 = <<1/6+5/2+25+20+5+20+20+20+20+20+30+15+5+35+75+75+75+75+40 + 75+80 + 75\n",
      "#### 75+5, 55+25 to 60 = <<75+25+15=60>>60\n",
      "#### 60<|endoftext|>\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "check(hypernet, control, tokenizer, eval_dataset[0], do_sample=True, temperature=1.0, max_new_tokens=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fde280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (control_name, control_param), (lm_name, lm_param) in zip(control.named_parameters(), hypernet.lm.named_parameters()):\n",
    "    if not (control_param.data.cpu() == lm_param.data.cpu()).all().item():\n",
    "        print(f\"{control_name} {lm_name} are different\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskweaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
