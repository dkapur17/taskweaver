{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81354538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvkapur/anaconda3/envs/taskweaver/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Dict, Tuple, Literal, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from einops import einsum, reduce\n",
    "from functools import partial\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from operator import attrgetter\n",
    "\n",
    "\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be3d862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "\n",
    "# login(token=os.environ['HF_TOKEN'])\n",
    "\n",
    "model = 'EleutherAI/pythia-70M-deduped'\n",
    "# model = 'google/gemma-3-270m-it'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4a3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = AutoModelForCausalLM.from_pretrained(model)\n",
    "lm = AutoModelForCausalLM.from_pretrained(model, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33919239",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"openai/gsm8k\", 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f69c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(examples):\n",
    "    results = {\n",
    "        'input_ids': [],\n",
    "        'attention_mask': [],\n",
    "        'labels': [],\n",
    "        'prompt_length': []\n",
    "    }\n",
    "\n",
    "    for prompt, completion in zip(examples['question'], examples['answer']):\n",
    "        # Tokenize prompt and completion separately\n",
    "        prompt_tokens = tokenizer(prompt, add_special_tokens=True)\n",
    "        completion_tokens = tokenizer(completion, add_special_tokens=False)  # Don't add special tokens again!\n",
    "        \n",
    "        # Concatenate the token IDs\n",
    "        input_ids = prompt_tokens['input_ids'] + completion_tokens['input_ids']\n",
    "        attention_mask = prompt_tokens['attention_mask'] + completion_tokens['attention_mask']\n",
    "        \n",
    "        # Create labels: mask prompt, keep completion\n",
    "        prompt_length = len(prompt_tokens['input_ids'])\n",
    "        labels = [-100] * prompt_length + completion_tokens['input_ids']\n",
    "        \n",
    "        results['input_ids'].append(input_ids)\n",
    "        results['attention_mask'].append(attention_mask)\n",
    "        results['labels'].append(labels)\n",
    "        results['prompt_length'].append(prompt_length)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2837597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List\n",
    "import torch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPromptLengths(DataCollatorForLanguageModeling):\n",
    "    \"\"\"\n",
    "    Data collator that handles padding and prompt lengths.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        # Extract prompt_lengths before processing\n",
    "        prompt_lengths = None\n",
    "        if features and 'prompt_length' in features[0]:\n",
    "            prompt_lengths = [f.pop('prompt_length') for f in features]\n",
    "        \n",
    "        # Manual padding since parent class is failing\n",
    "        batch = {}\n",
    "        \n",
    "        # Get max length in batch\n",
    "        max_length = max(len(f['input_ids']) for f in features)\n",
    "        \n",
    "        # Pad each sequence\n",
    "        input_ids = []\n",
    "        attention_mask = []\n",
    "        labels = []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Convert to list if needed\n",
    "            input_id = feature['input_ids']\n",
    "            if isinstance(input_id, torch.Tensor):\n",
    "                input_id = input_id.tolist()\n",
    "            \n",
    "            attn_mask = feature['attention_mask']\n",
    "            if isinstance(attn_mask, torch.Tensor):\n",
    "                attn_mask = attn_mask.tolist()\n",
    "            \n",
    "            label = feature['labels']\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.tolist()\n",
    "            \n",
    "            # Calculate padding length\n",
    "            padding_length = max_length - len(input_id)\n",
    "            \n",
    "            # Pad sequences (padding on the right for causal LM)\n",
    "            input_ids.append(input_id + [self.tokenizer.pad_token_id] * padding_length)\n",
    "            attention_mask.append(attn_mask + [0] * padding_length)\n",
    "            labels.append(label + [-100] * padding_length)  # -100 is ignored in loss\n",
    "        \n",
    "        # Convert to tensors\n",
    "        batch['input_ids'] = torch.tensor(input_ids, dtype=torch.long)\n",
    "        batch['attention_mask'] = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        batch['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        # Add prompt_lengths back\n",
    "        if prompt_lengths is not None:\n",
    "            batch['prompt_lengths'] = torch.tensor(prompt_lengths, dtype=torch.long)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d754eda5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing: 14946 examples [00:01, 4789.65 examples/s]        \n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset['train'].map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc='Tokenizing'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeec1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLoraLinear(nn.Linear):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            lora_rank: int,\n",
    "            lora_alpha: int,\n",
    "            lora_dropout: float = 0.0,\n",
    "            bias: bool = True,\n",
    "            device=None,\n",
    "            dtype=None\n",
    "    ):\n",
    "        super().__init__(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias is not None,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        assert lora_rank > 0, \"Use nn.Linear for Non-Lora Layer\"\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.lora_scaling = lora_alpha/lora_rank\n",
    "         \n",
    "\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def set_lora_paramters(self, A: torch.Tensor, B: torch.Tensor) -> None:\n",
    "        self.A = A # [batch_size x rank x input_dim]\n",
    "        self.B = B # [batch_size x output_dim x rank]\n",
    "\n",
    "    def replicate(self, target: nn.Linear) -> None:\n",
    "        assert isinstance(target, nn.Linear), \"Can only replicate nn.Linear\"\n",
    "\n",
    "        self.weight.data = target.weight.data\n",
    "        if self.bias is not None:\n",
    "            self.bias.data = target.bias.data\n",
    "\n",
    "    def unset_lora_parameters(self) -> None:\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # input: [batch_size x seq_len x input_dim]\n",
    "        \n",
    "        if self.A is None:\n",
    "            return F.linear(input, self.weight, self.bias)\n",
    "        \n",
    "        # Sanity check\n",
    "        batch_size = input.size(0)\n",
    "        if self.A.size(0) != batch_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size mismatch! Input batch_size={batch_size}, \"\n",
    "                f\"but LoRA A has batch_size={self.A.size(0)}. \"\n",
    "                f\"Old LoRA weights are being reused!\"\n",
    "            )\n",
    "\n",
    "        out_base = F.linear(input, self.weight, None)\n",
    "        out_delta = einsum(self.A, self.B, input, 'b r i, b o r, b s i -> b s o') # Instance-Level LoRA\n",
    "        \n",
    "        out =  out_base + self.lora_scaling * out_delta\n",
    "        if self.bias is not None:\n",
    "            out += self.bias    \n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        out = nn.Linear.extra_repr(self)\n",
    "        out += f', lora_rank={self.lora_rank}, lora_scaling={self.lora_scaling}, lora_dropout={self.lora_dropout}'\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9663cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskWeaver(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lm: AutoModelForCausalLM,\n",
    "            hidden_dim: int,\n",
    "            lora_rank: int,\n",
    "            lora_target_layers: List[str],\n",
    "            lora_alpha: float,\n",
    "            lora_dropout: float=0.0,\n",
    "            layers_module_name: str = 'layers'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.lora_target_layers = lora_target_layers\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # LLM config vals\n",
    "        self.lm_num_layers = self.lm.config.num_hidden_layers\n",
    "        self.lm_hidden_dim = self.lm.config.hidden_size\n",
    "\n",
    "\n",
    "        lm_layers_ref = self.get_layers_ref(layers_module_name)\n",
    "        assert isinstance(lm_layers_ref, nn.ModuleList), \"Layers must be an nn.ModuleList\"\n",
    "\n",
    "        dynamic_lora_fn = partial(DynamicLoraLinear, lora_rank=lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout, device=self.lm.device)\n",
    "        \n",
    "        self.module_references, self.in_features, self.out_features = self.replace_linears(self.lora_target_layers, lm_layers_ref, dynamic_lora_fn)\n",
    "        \n",
    "        self.semantic_proj = nn.Linear(self.lm_hidden_dim, hidden_dim)\n",
    "\n",
    "        self.module_embedding = nn.Embedding(len(lora_target_layers), hidden_dim)\n",
    "        self.matrix_embedding = nn.Embedding(2, hidden_dim)\n",
    "        self.layer_embedding = nn.Embedding(self.lm_num_layers, hidden_dim)\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            module_name: nn.ModuleDict({\n",
    "                'A': nn.Linear(hidden_dim, self.in_features[module_name] * self.lora_rank),\n",
    "                'B': nn.Linear(hidden_dim, self.out_features[module_name] * self.lora_rank)\n",
    "            }) for module_name in self.lora_target_layers\n",
    "        })\n",
    "\n",
    "        self._init_weights()\n",
    "        self._freeze_lm()\n",
    "\n",
    "    def _freeze_lm(self):\n",
    "\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize MLP layers with smaller weights\n",
    "        for module in [self.semantic_proj, self.mlp]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "        \n",
    "        # Initialize output heads to produce small initial LoRA weights\n",
    "        for module_name in self.lora_target_layers:\n",
    "            for matrix_name in ['A', 'B']:\n",
    "                head = self.heads[module_name][matrix_name]\n",
    "                nn.init.zeros_(head.weight)  # Start with zero weights\n",
    "                \n",
    "                if matrix_name == 'A':\n",
    "                    # Small random bias for A matrix\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.uniform_(head.bias, -1/np.sqrt(self.in_features[module_name]), \n",
    "                                                    1/np.sqrt(self.in_features[module_name]))\n",
    "                else:  # B matrix\n",
    "                    # Zero bias for B matrix (standard LoRA init)\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.zeros_(head.bias)\n",
    "\n",
    "    def get_layers_ref(self, layers_module_name:str) -> nn.Module:\n",
    "\n",
    "        for name, _ in self.lm.named_modules():\n",
    "            if not name or name.count('.') == 0:\n",
    "                continue\n",
    "            path, attribute = name.rsplit(\".\", 1)\n",
    "            if attribute == layers_module_name:\n",
    "                return attrgetter(name)(self.lm)\n",
    "\n",
    "\n",
    "    def replace_linears(self, lora_target_layers: List[str], lm_layers_ref:nn.ModuleList, dynamic_lora_fn:callable) -> Tuple[List[Dict[str, DynamicLoraLinear]], Dict[str, int], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Replaces target Linear layers with DynamicLoraLinears and return references, and module shapes\n",
    "\n",
    "        Args:\n",
    "            lora_target_layers (List[str])\n",
    "        \"\"\"\n",
    "\n",
    "        references = [{} for _ in range(self.lm_num_layers)]\n",
    "        in_features = {}\n",
    "        out_features = {}\n",
    "\n",
    "        for i, layer in enumerate(lm_layers_ref):\n",
    "            \n",
    "            for name, _ in layer.named_modules():\n",
    "                if not name or name.count('.') == 0:\n",
    "                    continue\n",
    "                \n",
    "                path, attribute = name.rsplit('.', 1)\n",
    "                if attribute not in lora_target_layers:\n",
    "                    continue\n",
    "                \n",
    "                parent_ref = attrgetter(path)(layer)\n",
    "                linear_ref = getattr(parent_ref, attribute)\n",
    "                assert isinstance(linear_ref, nn.Linear), \"Can only adapt nn.Linear layers\"\n",
    "                in_features[attribute] = linear_ref.in_features\n",
    "                out_features[attribute] = linear_ref.out_features\n",
    "                dynamic_lora_layer = dynamic_lora_fn(in_features=linear_ref.in_features, out_features=linear_ref.out_features, bias=linear_ref.bias is not None)\n",
    "                dynamic_lora_layer.replicate(linear_ref)\n",
    "                setattr(parent_ref, attribute, dynamic_lora_layer)\n",
    "                references[i][attribute] = getattr(parent_ref, attribute)\n",
    "\n",
    "        \n",
    "        return references, in_features, out_features\n",
    "\n",
    "    def _hypernet_forward(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor,\n",
    "            prompt_lengths: Optional[torch.Tensor] = None\n",
    "    ) -> List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]:\n",
    "\n",
    "\n",
    "        self.clear_lora_weights()        \n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        if prompt_lengths is not None:\n",
    "            seq_len = attention_mask.shape[1]\n",
    "            positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) # [1, seq_len]\n",
    "            prompt_lengths_expanded = prompt_lengths.unsqueeze(1) # [batch_size, 1]\n",
    "            prompt_mask = (positions < prompt_lengths_expanded).long()\n",
    "        else:\n",
    "            prompt_mask = attention_mask\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.lm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "            if prompt_lengths is not None:\n",
    "                last_prompt_indices = prompt_lengths - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_prompt_indices\n",
    "                ] # [batch, hidden]\n",
    "            else:\n",
    "                last_indices = attention_mask.sum(dim=1) - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_indices\n",
    "                ]\n",
    "    \n",
    "        semantic_embedding = self.semantic_proj(semantic_embedding.detach())\n",
    "\n",
    "        lora_weights = []\n",
    "\n",
    "        for layer_idx in range(self.lm_num_layers):\n",
    "            \n",
    "            layer_dict = {}\n",
    "            layer_emb = self.layer_embedding.weight[layer_idx:layer_idx+1]\n",
    "\n",
    "            for module_idx, module_name in enumerate(self.lora_target_layers):\n",
    "\n",
    "                module_dict = {}\n",
    "                module_emb = self.module_embedding.weight[module_idx:module_idx+1]\n",
    "\n",
    "                for matrix_idx, matrix_name in enumerate(['A', 'B']):\n",
    "\n",
    "                    matrix_emb = self.matrix_embedding.weight[matrix_idx:matrix_idx+1]\n",
    "\n",
    "                    combined_emb = semantic_embedding + layer_emb + module_emb + matrix_emb\n",
    "                    combined_emb = self.mlp(combined_emb)\n",
    "                    flat_weight = self.heads[module_name][matrix_name](combined_emb)\n",
    "\n",
    "                    if matrix_name == 'A':\n",
    "                        weight = flat_weight.view(batch_size, self.lora_rank, self.in_features[module_name])\n",
    "                    else:\n",
    "                        weight = flat_weight.view(batch_size, self.out_features[module_name], self.lora_rank)\n",
    "                    \n",
    "                    module_dict[matrix_name] = weight\n",
    "\n",
    "                layer_dict[module_name] = module_dict\n",
    "            \n",
    "            lora_weights.append(layer_dict)\n",
    "\n",
    "        return lora_weights\n",
    "    \n",
    "    \n",
    "    def inject_lora_weights(self, lora_weights: List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]) -> None:\n",
    "\n",
    "        for i, layer_dict in enumerate(self.module_references):\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].set_lora_paramters(**lora_weights[i][module_name])\n",
    "\n",
    "    def clear_lora_weights(self) -> None:\n",
    "        for layer_dict in self.module_references:\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].unset_lora_parameters()\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids:torch.Tensor, \n",
    "            attention_mask:torch.Tensor, \n",
    "            labels:Optional[torch.Tensor]=None, \n",
    "            prompt_lengths:Optional[torch.Tensor]=None,\n",
    "            skip_hypernet: bool = False\n",
    "        ):\n",
    "        \n",
    "        if not skip_hypernet:\n",
    "            lora_weights = self._hypernet_forward(input_ids=input_ids, attention_mask=attention_mask, prompt_lengths=prompt_lengths)\n",
    "            self.inject_lora_weights(lora_weights)\n",
    "        outputs = self.lm(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        prompt_lengths: Optional[torch.Tensor] = None,\n",
    "        **generation_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate text using the task-adapted model.\n",
    "\n",
    "        This method first generates LoRA weights using the hypernetwork,\n",
    "        injects them into the model, and then runs generation.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask (optional)\n",
    "            prompt_lengths: Length of prompts in each sequence (optional)\n",
    "            **generation_kwargs: Additional arguments passed to the LM's generate method\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Generate LoRA weights based on the prompt\n",
    "        lora_weights = self._hypernet_forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            prompt_lengths=prompt_lengths\n",
    "        )\n",
    "\n",
    "        # Inject LoRA weights into the model\n",
    "        self.inject_lora_weights(lora_weights)\n",
    "\n",
    "        # Generate using the adapted model\n",
    "        try:\n",
    "            outputs = self.lm.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        finally:\n",
    "            # Clear LoRA weights after generation\n",
    "            self.clear_lora_weights()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.lm.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbbf977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = TaskWeaver(\n",
    "    lm,\n",
    "    hidden_dim=256,\n",
    "    lora_rank=2, \n",
    "    lora_target_layers=['query_key_value'],\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e79c1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0114, -0.0078,  0.0025,  ..., -0.0084,  0.0311, -0.0108],\n",
       "        [ 0.0106, -0.0140,  0.0265,  ...,  0.0295, -0.0125, -0.0290],\n",
       "        [-0.0026,  0.0392,  0.0035,  ..., -0.0137,  0.0038,  0.0132],\n",
       "        ...,\n",
       "        [-0.0016,  0.0728,  0.0597,  ...,  0.0257, -0.0138, -0.0171],\n",
       "        [ 0.0337,  0.0347,  0.0022,  ..., -0.0211,  0.0070,  0.0025],\n",
       "        [-0.0079,  0.0007,  0.0436,  ..., -0.0582,  0.0079, -0.0101]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control.gpt_neox.layers[0].attention.query_key_value.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9320b699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0114, -0.0078,  0.0025,  ..., -0.0084,  0.0311, -0.0108],\n",
       "        [ 0.0106, -0.0140,  0.0265,  ...,  0.0295, -0.0125, -0.0290],\n",
       "        [-0.0026,  0.0392,  0.0035,  ..., -0.0137,  0.0038,  0.0132],\n",
       "        ...,\n",
       "        [-0.0016,  0.0728,  0.0597,  ...,  0.0257, -0.0138, -0.0171],\n",
       "        [ 0.0337,  0.0347,  0.0022,  ..., -0.0211,  0.0070,  0.0025],\n",
       "        [-0.0079,  0.0007,  0.0436,  ..., -0.0582,  0.0079, -0.0101]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypernet.lm.gpt_neox.layers[0].attention.query_key_value.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0854fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71745536\n",
      "1318912\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in hypernet.parameters())\n",
    "trainable_params = sum(p.numel() for p in hypernet.parameters() if p.requires_grad)\n",
    "\n",
    "print(total_params)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44909484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPromptLengths(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./taskweaver_output',\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    fp16=False,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    dataloader_pin_memory=False, \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=hypernet,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb90e1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='468' max='468' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [468/468 05:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.543300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.129900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.852500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.682200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.510100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.418600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.412000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.280300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.180500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.117000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.114600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.133800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.149000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.035100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.075900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.024900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.022500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.014900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.053900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.912000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.909300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.003900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.016600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.936400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.903500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.909100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=468, training_loss=2.2267726303165793, metrics={'train_runtime': 355.4146, 'train_samples_per_second': 21.026, 'train_steps_per_second': 1.317, 'total_flos': 0.0, 'train_loss': 2.2267726303165793, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6500918b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True, device='mps:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(control.gpt_neox.layers[1].attention.query_key_value.weight.to('mps') == hypernet.lm.gpt_neox.layers[1].attention.query_key_value.weight).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "115ec597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\n",
      "Answer: Natalia sold 48/2 = <<48/2=24>>24 clips in May.\n",
      "Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\n",
      "#### 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HN Out: She sold half to her friends in March, so you counted three of those clips from April -48 = <<3D/48=3>>3 clips.\n",
      "#### 3D clips\n",
      "There were a total of 18.0% of total YouTube clips on April\n",
      "Julien sold 50% of their friends in March -48 = <<50%+50=80>>80 clips\n",
      "#### 80% of YouTube clips on April\n",
      "Julien sold 40% or 80% of YouTube clips on April\n",
      "Julien sold 55% or 80% of the time on April\n",
      "Julien sold 40% or 81% to her friends in the\n",
      "HN LM Out: \n",
      "\n",
      "In January 2016, Natalia and Natalia moved into a location of the Bremerton branch of Ore. Natalia and Natalia met her parentsâ€™s only aunt, Natalia Dravet, at AHL (the National Hockey League). Natalia and Natalia sold six clips for each one and five songs over seven years; however, Natalia and Natalia eventually had one. Natalia first hit the spot in March 2016, and she also recorded the most successful song written by Natalia and Natalia, who recorded the song's run by Natalia:\n",
      "\n",
      "In a move to Nashville to continue\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][0]\n",
    "question = sample['question']\n",
    "answer = sample['answer']\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "inputs = tokenizer(question, return_tensors='pt')\n",
    "inputs = {k:v.to(hypernet.device) for k,v in inputs.items()}\n",
    "\n",
    "hn_out = hypernet.generate(**inputs, max_new_tokens=128, temperature=1.0, do_sample=True).squeeze()\n",
    "print(f\"HN Out: {tokenizer.decode(hn_out)[len(question):]}\")\n",
    "\n",
    "hn_model_out = hypernet.lm.generate(**inputs, max_new_tokens=128, temperature=1.0, do_sample=True).squeeze()\n",
    "print(f\"HN LM Out: {tokenizer.decode(hn_model_out)[len(question):]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskweaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
