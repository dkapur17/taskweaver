{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5aaa16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dhruvkapur/anaconda3/envs/taskweaver/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from trl.trainer.sft_trainer import DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "from typing import Optional, List, Dict, Tuple, Literal, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from einops import einsum, reduce\n",
    "from functools import partial\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from operator import attrgetter\n",
    "from dsconf import DatasetConfig\n",
    "\n",
    "\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d4119ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'EleutherAI/pythia-70m-deduped'\n",
    "# model = 'google/gemma-3-270m'\n",
    "# model = 'google/gemma-3-270m-it'\n",
    "# model = 'Qwen/Qwen3-0.6B'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51946a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "control = AutoModelForCausalLM.from_pretrained(model)\n",
    "lm = AutoModelForCausalLM.from_pretrained(model, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "is_chat = tokenizer.chat_template is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8490fa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 7473/7473 [00:00<00:00, 1391027.99 examples/s]\n",
      "Generating test split: 100%|██████████| 1319/1319 [00:00<00:00, 626036.77 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 552152.37 examples/s]\n",
      "Map: 100%|██████████| 7473/7473 [00:00<00:00, 91569.70 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 345013.22 examples/s]\n",
      "Map: 100%|██████████| 1319/1319 [00:00<00:00, 71432.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_config = DatasetConfig.from_dataset_path('openai/gsm8k', 'main')\n",
    "train_dataset = dataset_config.load_and_process(is_chat, 'train', enable_thinking=False)\n",
    "eval_dataset = dataset_config.load_and_process(is_chat, 'test', enable_thinking=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ef0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicLoraLinear(nn.Linear):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            lora_rank: int,\n",
    "            lora_alpha: int,\n",
    "            lora_dropout: float = 0.0,\n",
    "            bias: bool = True,\n",
    "            device=None,\n",
    "            dtype=None\n",
    "    ):\n",
    "        \n",
    "        super().__init__(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias,\n",
    "            device=device,\n",
    "            dtype=dtype\n",
    "        )\n",
    "\n",
    "        assert lora_rank > 0, \"Use nn.Linear for Non-Lora Layer\"\n",
    "\n",
    "        self.lora_rank = lora_rank\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.lora_scaling = lora_alpha/lora_rank\n",
    "         \n",
    "\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def set_lora_paramters(self, A: torch.Tensor, B: torch.Tensor) -> None:\n",
    "        self.A = A # [batch_size x rank x input_dim]\n",
    "        self.B = B # [batch_size x output_dim x rank]\n",
    "\n",
    "    def replicate(self, target: nn.Linear) -> None:\n",
    "        assert isinstance(target, nn.Linear), \"Can only replicate nn.Linear\"\n",
    "\n",
    "        self.weight.data = target.weight.data\n",
    "        if self.bias is not None:\n",
    "            self.bias.data = target.bias.data\n",
    "\n",
    "    def unset_lora_parameters(self) -> None:\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # input: [batch_size x seq_len x input_dim]\n",
    "        \n",
    "        if self.A is None:\n",
    "            return F.linear(input, self.weight, self.bias)\n",
    "        \n",
    "        # Sanity check\n",
    "        batch_size = input.size(0)\n",
    "        if self.A.size(0) != batch_size:\n",
    "            raise RuntimeError(\n",
    "                f\"Batch size mismatch! Input batch_size={batch_size}, \"\n",
    "                f\"but LoRA A has batch_size={self.A.size(0)}. \"\n",
    "                f\"Old LoRA weights are being reused!\"\n",
    "            )\n",
    "\n",
    "        out_base = F.linear(input, self.weight, None)\n",
    "        out_delta = einsum(self.A, self.B, input, 'b r i, b o r, b s i -> b s o') # Instance-Level LoRA\n",
    "        \n",
    "        out =  out_base + self.lora_scaling * out_delta\n",
    "        if self.bias is not None:\n",
    "            out += self.bias    \n",
    "        return out\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        out = nn.Linear.extra_repr(self)\n",
    "        out += f', lora_rank={self.lora_rank}, lora_scaling={self.lora_scaling}, lora_dropout={self.lora_dropout}'\n",
    "        return out\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d00dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskWeaver(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            lm: AutoModelForCausalLM,\n",
    "            hidden_dim: int,\n",
    "            lora_rank: int,\n",
    "            lora_target_layers: List[str],\n",
    "            lora_alpha: float,\n",
    "            lora_dropout: float=0.0,\n",
    "            layers_module_name: str = 'layers'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.lm = lm\n",
    "        self.lora_target_layers = lora_target_layers\n",
    "        self.lora_rank = lora_rank\n",
    "\n",
    "        # LLM config vals\n",
    "        self.lm_num_layers = self.lm.config.num_hidden_layers\n",
    "        self.lm_hidden_dim = self.lm.config.hidden_size\n",
    "\n",
    "\n",
    "        lm_layers_ref = self.get_layers_ref(layers_module_name)\n",
    "        assert isinstance(lm_layers_ref, nn.ModuleList), \"Layers must be an nn.ModuleList\"\n",
    "\n",
    "        dynamic_lora_fn = partial(DynamicLoraLinear, lora_rank=lora_rank, lora_alpha=lora_alpha, lora_dropout=lora_dropout, device=self.lm.device)\n",
    "        \n",
    "        self.module_references, self.in_features, self.out_features = self.replace_linears(self.lora_target_layers, lm_layers_ref, dynamic_lora_fn)\n",
    "        \n",
    "        self.semantic_proj = nn.Linear(self.lm_hidden_dim, hidden_dim)\n",
    "\n",
    "        self.module_embedding = nn.Embedding(len(lora_target_layers), hidden_dim)\n",
    "        self.matrix_embedding = nn.Embedding(2, hidden_dim)\n",
    "        self.layer_embedding = nn.Embedding(self.lm_num_layers, hidden_dim)\n",
    "\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "        )\n",
    "\n",
    "        self.heads = nn.ModuleDict({\n",
    "            module_name: nn.ModuleDict({\n",
    "                'A': nn.Linear(hidden_dim, self.in_features[module_name] * self.lora_rank),\n",
    "                'B': nn.Linear(hidden_dim, self.out_features[module_name] * self.lora_rank)\n",
    "            }) for module_name in self.lora_target_layers\n",
    "        })\n",
    "\n",
    "        self._init_weights()\n",
    "        self._freeze_lm()\n",
    "\n",
    "    def _freeze_lm(self):\n",
    "\n",
    "        for param in self.lm.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize MLP layers with smaller weights\n",
    "        for module in [self.semantic_proj, self.mlp]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "        \n",
    "        # Initialize output heads to produce small initial LoRA weights\n",
    "        for module_name in self.lora_target_layers:\n",
    "            for matrix_name in ['A', 'B']:\n",
    "                head = self.heads[module_name][matrix_name]\n",
    "                nn.init.zeros_(head.weight)  # Start with zero weights\n",
    "                \n",
    "                if matrix_name == 'A':\n",
    "                    # Small random bias for A matrix\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.uniform_(head.bias, -1/np.sqrt(self.in_features[module_name]), \n",
    "                                                    1/np.sqrt(self.in_features[module_name]))\n",
    "                else:  # B matrix\n",
    "                    # Zero bias for B matrix (standard LoRA init)\n",
    "                    if hasattr(head, 'bias') and head.bias is not None:\n",
    "                        nn.init.zeros_(head.bias)\n",
    "\n",
    "    def get_layers_ref(self, layers_module_name:str) -> nn.Module:\n",
    "\n",
    "        for name, _ in self.lm.named_modules():\n",
    "            if not name or name.count('.') == 0:\n",
    "                continue\n",
    "            path, attribute = name.rsplit(\".\", 1)\n",
    "            if attribute == layers_module_name:\n",
    "                return attrgetter(name)(self.lm)\n",
    "\n",
    "\n",
    "    def replace_linears(self, lora_target_layers: List[str], lm_layers_ref:nn.ModuleList, dynamic_lora_fn:callable) -> Tuple[List[Dict[str, DynamicLoraLinear]], Dict[str, int], Dict[str, int]]:\n",
    "        \"\"\"\n",
    "        Replaces target Linear layers with DynamicLoraLinears and return references, and module shapes\n",
    "\n",
    "        Args:\n",
    "            lora_target_layers (List[str])\n",
    "        \"\"\"\n",
    "\n",
    "        references = [{} for _ in range(self.lm_num_layers)]\n",
    "        in_features = {}\n",
    "        out_features = {}\n",
    "\n",
    "        for i, layer in enumerate(lm_layers_ref):\n",
    "            \n",
    "            for name, _ in layer.named_modules():\n",
    "                if not name or name.count('.') == 0:\n",
    "                    continue\n",
    "                \n",
    "                path, attribute = name.rsplit('.', 1)\n",
    "                if attribute not in lora_target_layers:\n",
    "                    continue\n",
    "                \n",
    "                parent_ref = attrgetter(path)(layer)\n",
    "                linear_ref = getattr(parent_ref, attribute)\n",
    "                assert isinstance(linear_ref, nn.Linear), \"Can only adapt nn.Linear layers\"\n",
    "                in_features[attribute] = linear_ref.in_features\n",
    "                out_features[attribute] = linear_ref.out_features\n",
    "                dynamic_lora_layer = dynamic_lora_fn(in_features=linear_ref.in_features, out_features=linear_ref.out_features, bias=(linear_ref.bias is not None))\n",
    "                dynamic_lora_layer.replicate(linear_ref)\n",
    "                setattr(parent_ref, attribute, dynamic_lora_layer)\n",
    "                references[i][attribute] = getattr(parent_ref, attribute)\n",
    "\n",
    "        \n",
    "        return references, in_features, out_features\n",
    "\n",
    "    def _hypernet_forward(\n",
    "            self,\n",
    "            input_ids: torch.Tensor,\n",
    "            attention_mask: torch.Tensor,\n",
    "            prompt_length: Optional[torch.Tensor] = None\n",
    "    ) -> List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]:\n",
    "\n",
    "\n",
    "        self.clear_lora_weights()        \n",
    "\n",
    "        batch_size = input_ids.shape[0]\n",
    "\n",
    "        if prompt_length is not None:\n",
    "            seq_len = attention_mask.shape[1]\n",
    "            positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0) # [1, seq_len]\n",
    "            prompt_length_expanded = prompt_length.unsqueeze(1) # [batch_size, 1]\n",
    "            prompt_mask = (positions < prompt_length_expanded).long()\n",
    "        else:\n",
    "            prompt_mask = attention_mask\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.lm(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=prompt_mask,\n",
    "                output_hidden_states=True\n",
    "            )\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "            if prompt_length is not None:\n",
    "                last_prompt_indices = prompt_length - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_prompt_indices\n",
    "                ] # [batch, hidden]\n",
    "            else:\n",
    "                last_indices = attention_mask.sum(dim=1) - 1\n",
    "                semantic_embedding = last_hidden[\n",
    "                    torch.arange(batch_size, device=last_hidden.device),\n",
    "                    last_indices\n",
    "                ]\n",
    "    \n",
    "        semantic_embedding = self.semantic_proj(semantic_embedding.detach())\n",
    "\n",
    "        lora_weights = []\n",
    "\n",
    "        for layer_idx in range(self.lm_num_layers):\n",
    "            \n",
    "            layer_dict = {}\n",
    "            layer_emb = self.layer_embedding.weight[layer_idx:layer_idx+1]\n",
    "\n",
    "            for module_idx, module_name in enumerate(self.lora_target_layers):\n",
    "\n",
    "                module_dict = {}\n",
    "                module_emb = self.module_embedding.weight[module_idx:module_idx+1]\n",
    "\n",
    "                for matrix_idx, matrix_name in enumerate(['A', 'B']):\n",
    "\n",
    "                    matrix_emb = self.matrix_embedding.weight[matrix_idx:matrix_idx+1]\n",
    "\n",
    "                    combined_emb = semantic_embedding + layer_emb + module_emb + matrix_emb\n",
    "                    combined_emb = self.mlp(combined_emb)\n",
    "                    flat_weight = self.heads[module_name][matrix_name](combined_emb)\n",
    "\n",
    "                    if matrix_name == 'A':\n",
    "                        weight = flat_weight.view(batch_size, self.lora_rank, self.in_features[module_name])\n",
    "                    else:\n",
    "                        weight = flat_weight.view(batch_size, self.out_features[module_name], self.lora_rank)\n",
    "                    \n",
    "                    module_dict[matrix_name] = weight\n",
    "\n",
    "                layer_dict[module_name] = module_dict\n",
    "            \n",
    "            lora_weights.append(layer_dict)\n",
    "\n",
    "        return lora_weights\n",
    "    \n",
    "    \n",
    "    def inject_lora_weights(self, lora_weights: List[Dict[str, Dict[Literal['A', 'B'], torch.Tensor]]]) -> None:\n",
    "\n",
    "        for i, layer_dict in enumerate(self.module_references):\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].set_lora_paramters(**lora_weights[i][module_name])\n",
    "\n",
    "    def clear_lora_weights(self) -> None:\n",
    "        for layer_dict in self.module_references:\n",
    "            for module_name in layer_dict:\n",
    "                layer_dict[module_name].unset_lora_parameters()\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            input_ids:torch.Tensor, \n",
    "            attention_mask:torch.Tensor, \n",
    "            labels:Optional[torch.Tensor]=None, \n",
    "            prompt_length:Optional[torch.Tensor]=None,\n",
    "            skip_hypernet: bool = False,\n",
    "            **lm_kwargs\n",
    "        ):\n",
    "        \n",
    "        if not skip_hypernet:\n",
    "            lora_weights = self._hypernet_forward(input_ids=input_ids, attention_mask=attention_mask, prompt_length=prompt_length)\n",
    "            self.inject_lora_weights(lora_weights)\n",
    "        outputs = self.lm(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **lm_kwargs)\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        prompt_length: Optional[torch.Tensor] = None,\n",
    "        **generation_kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Generate text using the task-adapted model.\n",
    "\n",
    "        This method first generates LoRA weights using the hypernetwork,\n",
    "        injects them into the model, and then runs generation.\n",
    "\n",
    "        Args:\n",
    "            input_ids: Input token IDs\n",
    "            attention_mask: Attention mask (optional)\n",
    "            prompt_length: Length of prompts in each sequence (optional)\n",
    "            **generation_kwargs: Additional arguments passed to the LM's generate method\n",
    "\n",
    "        Returns:\n",
    "            Generated token IDs\n",
    "        \"\"\"\n",
    "        # Create attention mask if not provided\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones_like(input_ids)\n",
    "\n",
    "        # Generate LoRA weights based on the prompt\n",
    "        lora_weights = self._hypernet_forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            prompt_length=prompt_length\n",
    "        )\n",
    "\n",
    "        # Inject LoRA weights into the model\n",
    "        self.inject_lora_weights(lora_weights)\n",
    "\n",
    "        # Generate using the adapted model\n",
    "        try:\n",
    "            outputs = self.lm.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **generation_kwargs\n",
    "            )\n",
    "        finally:\n",
    "            # Clear LoRA weights after generation\n",
    "            self.clear_lora_weights()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.lm.device\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        return self.lm.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9748cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypernet = TaskWeaver(\n",
    "    lm,\n",
    "    hidden_dim=256,\n",
    "    lora_rank=2, \n",
    "    lora_target_layers=['query_key_value'],\n",
    "    # lora_target_layers=['q_proj', 'v_proj'],\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bffda87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71745536\n",
      "1318912\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in hypernet.parameters())\n",
    "trainable_params = sum(p.numel() for p in hypernet.parameters() if p.requires_grad)\n",
    "\n",
    "print(total_params)\n",
    "print(trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f22df0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorWithPromptLenghts(DataCollatorForLanguageModeling):\n",
    "\n",
    "    def __call__(self, examples:List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        batch = super().torch_call(examples)\n",
    "        batch['prompt_length'] = (batch['labels'] != -100).int().argmax(dim=1)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92855fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "pad_token_id = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "\n",
    "collator = DataCollatorWithPromptLenghts(pad_token_id=pad_token_id)\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    num_train_epochs = 1.0,\n",
    "    per_device_train_batch_size = 2,\n",
    "    gradient_accumulation_steps = 2,\n",
    "    learning_rate = 5e-5,\n",
    "    bf16 = False,\n",
    "    logging_steps = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c412f969",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding EOS to train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 65653.37 examples/s]\n",
      "Tokenizing train dataset: 100%|██████████| 7473/7473 [00:02<00:00, 2725.80 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 7473/7473 [00:00<00:00, 1117195.39 examples/s]\n",
      "Adding EOS to eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 63520.14 examples/s]\n",
      "Tokenizing eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 2560.30 examples/s]\n",
      "Truncating eval dataset: 100%|██████████| 1319/1319 [00:00<00:00, 492546.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=hypernet,\n",
    "    data_collator=collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    args=training_arguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd6519d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 0}.\n",
      "/Users/dhruvkapur/anaconda3/envs/taskweaver/lib/python3.12/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1869' max='1869' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1869/1869 05:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>17.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>17.625500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>15.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>15.943600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>18.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>15.457800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>11.994000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>4.459100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>3.789100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>3.739300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>3.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>3.356500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>3.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>3.559800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>3.384000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>3.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>3.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>3.168500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>3.333600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>3.386600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>3.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>3.058300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>3.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>3.022800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>3.136200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>3.162700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.997900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.975200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.688400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.519400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.662700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.919900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>3.013300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>3.642900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.806500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.565500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.642000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.781500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.576800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.629000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.596400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.594100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>2.500200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.548800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>2.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.484800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>2.394000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.820900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>2.645400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.441600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.286200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>2.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>2.491800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>2.516800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>2.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.373400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>2.285300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>2.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.458700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>2.320400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>2.397600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.405800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>2.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.384300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>2.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>2.357900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>2.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.300200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>2.362600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.344600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>2.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.265400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>2.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>2.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.131100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.341600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>2.178700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>2.205600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>2.371800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>2.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.124400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.183900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>2.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>2.104000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.085400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>2.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.120600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>2.310300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.120200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.165200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>2.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.440500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>2.043800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>2.203900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>2.194200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.275100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.087500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>2.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>2.156900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.078900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>2.162200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>1.966000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>2.081400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>2.100300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>2.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.971300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>2.131700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.192300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>2.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.151400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.200700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>2.089300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>2.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.016400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>2.121900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>2.095400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.068900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.060300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>2.211800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.113500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>1.950800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.143500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>2.026600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.059500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>2.182000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.179100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>2.107600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>1.886000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>2.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>2.187000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.008200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.007200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1869, training_loss=3.027772908544974, metrics={'train_runtime': 352.6594, 'train_samples_per_second': 21.19, 'train_steps_per_second': 5.3, 'total_flos': 0.0, 'train_loss': 3.027772908544974, 'entropy': 2.7829650710610783, 'num_tokens': 1458952.0, 'mean_token_accuracy': 0.5495762334150427, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "365e1220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(hypernet, control, tokenizer, sample, **generation_kwargs):\n",
    "    inputs = tokenizer(text=sample['prompt'], return_tensors='pt')\n",
    "    hypernet_inputs = {k:v.to(hypernet.device) for k,v in inputs.items()}\n",
    "    control_inputs = {k:v.to(control.device) for k,v in inputs.items()}\n",
    "    \n",
    "    hypernet_output_ids = hypernet.generate(**hypernet_inputs, **generation_kwargs)[0]\n",
    "    control_output_ids = control.generate(**control_inputs, **generation_kwargs)[0]\n",
    "\n",
    "    hypernet_generated_ids = hypernet_output_ids[len(hypernet_inputs['input_ids'][0]):]\n",
    "    control_generated_ids = control_output_ids[len(control_inputs['input_ids'][0]):]\n",
    "\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Prompt\\n{sample['prompt']}\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Completion\\n{sample['completion']}\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Control Answer\\n{tokenizer.decode(control_generated_ids)}\")\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Hypernet Answer\\n{tokenizer.decode(hypernet_generated_ids)}\")\n",
    "    print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d3c9c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "Prompt\n",
      "Instruction: Analyze the given math problem, reason through it step by step, and provide the final answer in a new line starting with ####, for example: #### 72\n",
      "Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Answer:\n",
      "==============================\n",
      "Completion\n",
      " Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\n",
      "She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\n",
      "#### 18\n",
      "==============================\n",
      "Control Answer\n",
      " It’s worth the $2 per pair. In a similar vein, she is still a very short person but can still be a bit old.\n",
      "Question: Have the ducks put a bunch of birds through these holes or are there any possible pockets for her or her friends?\n",
      "Answer: Yes, but she still wants to find some bird for her friends who’ll make a few calls. No one can really understand, they’re just talking about a dozen birds! She likes birds, too.\n",
      "Question: Would the duck have only one of those birds, since there were no pockets for them. She’d like to see birds with an umbrella or an open window at the end of the year (she needs them in her basket, and if she can do one-by-one, can she give the remaining birds over to the farmers' market)?\n",
      "Answer: This is a common misconception.\n",
      "Question: It’s probably more likely that an older species might be at the farmers' market. It would be bad enough for something more modern than just going into the bin; I mean, what about a duck? Are those two two to three-to-five birds out of a 1-day-old duck? And, yes, they need a very long day?\n",
      "Answer: Of course, one of the two birds can take up the same position, so it would probably be wrong that there’s one turkey for this! But even if some birds are carrying a turkey at the farmers' market, they need a flight or two. And there might be more people who live long in the woods, some more people on their own.\n",
      "Question: Did they see the duck in the basket? The answer can be an old question!\n",
      "Answer: The duck has three chicks, but there’s one turkey for that!\n",
      "Answer: Yes! A turkey might be. If these birds were only at Farnsworth or Bighim, they would probably have four or four nest chicks. Just that one chick might have a few nests, but that might have a few nest chicks.\n",
      "Question: It is important that she has another hens, and to her to another chicken, a different one. She’s not asking questions or it’s her idea of one kind of turkey. So she’s asking questions like, “Do you have a chicken that you only want to eat right away?”\n",
      "Answer: Yes; I got my own idea of this!\n",
      "Answer: No, I’\n",
      "==============================\n",
      "Hypernet Answer\n",
      " Janet’s ducks lay 16 eggs per day every day\n",
      "Now, the remainder is a total sacrifice because she eats two of 21.10 eggs per day\n",
      "So, her chickens is $2.10 and the majority of her friends is $2.20.\n",
      "Now, they are also $10.20 between $1.5 and $7.25.\n",
      "Here’s $10 + 17.25 = $<<17.25=26.25>>26.\n",
      "If she eats 2.43 = $1+35=45.25 = 30.25, one way for the average of these deaths is to spend $20 + 45.25 = 3.25 + 45.25 = $14.25$.\n",
      "\n",
      "#### 14<|endoftext|>\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "check(hypernet, control, tokenizer, eval_dataset[0], do_sample=True, temperature=1.0, max_new_tokens=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taskweaver",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
